"""
Prompt ê¸°ë°˜ Agent ì‹¤í–‰ê¸°

ë‹¨ì¼ ì‹¤í–‰ê¸°ê°€ ëª¨ë“  Agent í”„ë¡¬í”„íŠ¸ë¥¼ ì½ê³  ì‹¤í–‰
"""
import subprocess
from pathlib import Path
from typing import Dict, Any, Optional
import json
import re


class AgentExecutor:
    """ë²”ìš© Agent ì‹¤í–‰ê¸°"""
    
    def __init__(self, prompts_dir: str = "agents/prompts"):
        """
        Args:
            prompts_dir: Agent í”„ë¡¬í”„íŠ¸ ë””ë ‰í† ë¦¬
        """
        self.prompts_dir = Path(prompts_dir)
        self.agents = {}
        self._load_agents()
    
    def _load_agents(self):
        """ëª¨ë“  Agent í”„ë¡¬í”„íŠ¸ ë¡œë“œ"""
        if not self.prompts_dir.exists():
            print(f"âš ï¸ Prompts ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.prompts_dir}")
            return
        
        for prompt_file in self.prompts_dir.glob("*.md"):
            agent_config = self._parse_agent_prompt(prompt_file)
            if agent_config:
                agent_name = agent_config['name']
                self.agents[agent_name] = agent_config
                print(f"âœ… Agent ë¡œë“œ: {agent_name} (v{agent_config.get('version', '1.0')})")
    
    def _parse_agent_prompt(self, file_path: Path) -> Optional[dict]:
        """
        Agent í”„ë¡¬í”„íŠ¸ íŒŒì‹± (YAML frontmatter + í”„ë¡¬í”„íŠ¸)
        
        Args:
            file_path: í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            Agent ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        try:
            content = file_path.read_text(encoding='utf-8')
            
            # YAML frontmatter ì¶”ì¶œ
            if content.startswith('---'):
                parts = content.split('---', 2)
                if len(parts) >= 3:
                    # YAML íŒŒì‹± (ìˆ˜ë™ - yaml ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´)
                    frontmatter_text = parts[1].strip()
                    frontmatter = {}
                    
                    for line in frontmatter_text.split('\n'):
                        if ':' in line:
                            key, value = line.split(':', 1)
                            key = key.strip()
                            value = value.strip()
                            
                            # íƒ€ì… ë³€í™˜
                            if value.lower() == 'true':
                                value = True
                            elif value.lower() == 'false':
                                value = False
                            elif value.replace('.', '').isdigit():
                                value = float(value) if '.' in value else int(value)
                            
                            frontmatter[key] = value
                    
                    prompt_content = parts[2].strip()
                    
                    return {
                        **frontmatter,
                        'prompt_template': prompt_content,
                        'file': str(file_path)
                    }
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ Agent íŒŒì‹± ì˜¤ë¥˜ ({file_path}): {e}")
            return None
    
    def execute_agent(self, 
                     agent_name: str, 
                     task: str, 
                     context: Dict[str, Any],
                     use_llm: bool = True) -> Dict[str, Any]:
        """
        Agent ì‹¤í–‰
        
        Args:
            agent_name: Agent ì´ë¦„
            task: ìˆ˜í–‰í•  ì‘ì—…
            context: ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°
            use_llm: LLM ì‚¬ìš© ì—¬ë¶€ (Falseë©´ Mock)
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼
        """
        from utils.logger import review_logger
        
        # Agent ë¡œë“œ
        if agent_name not in self.agents:
            raise ValueError(f"Agent '{agent_name}' not found. Available: {list(self.agents.keys())}")
        
        agent_config = self.agents[agent_name]
        review_logger.info(f"ğŸ¤– {agent_name} ì‹¤í–‰ ì¤‘...")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_prompt(agent_config, task, context)
        review_logger.debug(f"  í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ê¸€ì")
        
        if not use_llm:
            review_logger.info("  ğŸ“ Mock ëª¨ë“œ (LLM ë¯¸ì‚¬ìš©)")
            return self._mock_execution(agent_config, context)
        
        # LLM í˜¸ì¶œ
        review_logger.debug(f"  ëª¨ë¸: {agent_config.get('model', 'gemini-2.0-flash-exp')}")
        result = self._call_llm(
            prompt=prompt,
            model=agent_config.get('model', 'gemini-2.0-flash-exp'),
            temperature=agent_config.get('temperature', 0.7)
        )
        
        if 'error' in result:
            review_logger.error(f"  âŒ LLM ì˜¤ë¥˜: {result['error']}")
            review_logger.info("  ğŸ“ Mock ëª¨ë“œë¡œ ëŒ€ì²´...")
            return self._mock_execution(agent_config, context)
        
        review_logger.info(f"  âœ… {agent_name} ì™„ë£Œ")
        return result
    
    def _build_prompt(self, 
                     agent_config: dict, 
                     task: str, 
                     context: Dict[str, Any]) -> str:
        """í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        prompt_template = agent_config['prompt_template']
        
        # ì»¨í…ìŠ¤íŠ¸ ë³€ìˆ˜ ì¹˜í™˜
        for key, value in context.items():
            placeholder = f"{{{key}}}"
            if placeholder in prompt_template:
                prompt_template = prompt_template.replace(placeholder, str(value))
        
        # Task ì¶”ê°€
        full_prompt = f"""{prompt_template}

---

# Task
{task}

Context Data:
{json.dumps(context, indent=2, ensure_ascii=False)}
"""
        return full_prompt
    
    def _call_llm(self, 
                 prompt: str, 
                 model: str, 
                 temperature: float) -> Dict[str, Any]:
        """LLM í˜¸ì¶œ (Gemini CLI)"""
        try:
            result = subprocess.run(
                ["gemini", "chat", 
                 "--model", model,
                 "--temperature", str(temperature),
                 "--prompt", prompt],
                capture_output=True,
                text=True,
                timeout=60
            )
            
            if result.returncode != 0:
                return {"error": result.stderr}
            
            # JSON íŒŒì‹±
            output = result.stdout.strip()
            
            # Markdown ì½”ë“œ ë¸”ë¡ ì œê±°
            if "```json" in output:
                output = output.split("```json")[1].split("```")[0].strip()
            elif "```" in output:
                output = output.split("```")[1].split("```")[0].strip()
            
            try:
                return json.loads(output)
            except json.JSONDecodeError:
                # JSONì´ ì•„ë‹ˆë©´ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜
                return {"response": output}
                
        except subprocess.TimeoutExpired:
            return {"error": "Timeout (60ì´ˆ ì´ˆê³¼)"}
        except FileNotFoundError:
            return {"error": "Gemini CLI not found"}
        except Exception as e:
            return {"error": str(e)}
    
    def _mock_execution(self, agent_config: dict, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Mock ì‹¤í–‰ (LLM ì—†ì´ ê¸°ë³¸ ê²€ì¦)
        
        Review Agentì˜ ê²½ìš° ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì²´í¬
        """
        content = context.get('content', '')
        doc_type = context.get('document_type', 'spec')
        
        if agent_config['name'] == 'Review Agent':
            # ê°„ë‹¨í•œ ê²€ì¦
            checks = {}
            
            if doc_type == 'spec':
                checks = {
                    'has_user_stories': 'User Story' in content,
                    'has_requirements': 'Requirements' in content,
                    'has_success_criteria': 'Success Criteria' in content,
                    'min_length': len(content) > 500
                }
            elif doc_type == 'plan':
                checks = {
                    'has_phases': 'Phase' in content,
                    'has_structure': 'Project Structure' in content or 'êµ¬ì¡°' in content,
                    'has_verification': 'Verification' in content or 'Test' in content,
                    'min_length': len(content) > 800
                }
            
            score = sum(checks.values()) / len(checks) if checks else 0.5
            approved = score >= 0.7
            
            issues = [key for key, value in checks.items() if not value]
            
            return {
                "score": score,
                "approved": approved,
                "summary": f"Mock ê²€ì¦ ì™„ë£Œ ({sum(checks.values())}/{len(checks)} í†µê³¼)",
                "issues": [f"{issue} ë¯¸í†µê³¼" for issue in issues] if issues else [],
                "suggestions": ["ë” ìƒì„¸í•œ ë‚´ìš© ì¶”ê°€" if not approved else "ì—†ìŒ"],
                "strengths": [key for key, value in checks.items() if value]
            }
        
        # ë‹¤ë¥¸ AgentëŠ” ê¸°ë³¸ ì‘ë‹µ
        return {
            "response": f"{agent_config['name']} Mock ì‹¤í–‰ ì™„ë£Œ",
            "status": "success"
        }
    
    def list_agents(self) -> list[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ Agent ëª©ë¡"""
        return list(self.agents.keys())
    
    def reload_agents(self):
        """Agent ì¬ë¡œë“œ"""
        self.agents.clear()
        self._load_agents()
